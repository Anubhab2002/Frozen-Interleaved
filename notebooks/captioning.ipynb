{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a991c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f631bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor\n",
    "from frozen.experiment import Experiment\n",
    "from frozen.data import COCODataset, CC3MDataset, IMAGE_TOKEN, SPECIAL_TOKEN_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f5644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/teamspace/studios/this_studio/logs/run-sample-CCM/1.0.0/checkpoints/epoch=0-step=2.ckpt'\n"
     ]
    }
   ],
   "source": [
    "!ls /teamspace/studios/this_studio/logs/run-sample-CCM/1.0.0/checkpoints/epoch=0-step=2.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e903219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"/teamspace/studios/this_studio/logs/run-sample-CCM/1.0.0/checkpoints/epoch=0-step=2.ckpt\"\n",
    "# ckpt_path = osp.abspath(osp.expanduser(ckpt_path))\n",
    "device = 'cuda:0'\n",
    "experiment = Experiment.load_from_checkpoint(ckpt_path).half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbdbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = experiment.model.config['image_encoder']\n",
    "text_encoder = experiment.model.config['text_encoder']\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_encoder)\n",
    "num_image_tokens = experiment.model.config['num_image_tokens']\n",
    "if not IMAGE_TOKEN in tokenizer.all_special_tokens:\n",
    "    tokenizer.add_special_tokens(SPECIAL_TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c85ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:num_image_tokens = 2\n"
     ]
    }
   ],
   "source": [
    "data = CC3MDataset(\n",
    "    path='Frozen-Interleaved/frozen/datasets/conceptualcaptions/',\n",
    "    image_transform=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    num_image_tokens=num_image_tokens,\n",
    ")\n",
    "# )/teamspace/studios/this_studio/Frozen-Interleaved/frozen/datasets/conceptualcaptions/script/DownloadConceptualCaptions/sample_validation.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5d8942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-1.9467, -1.9638, -1.9809,  ..., -2.0152, -2.0152, -2.0152],\n",
       "           [-1.9638, -1.9467, -1.9467,  ..., -2.0152, -1.9980, -1.9809],\n",
       "           [-1.9467, -1.9295, -1.9295,  ..., -1.9980, -1.9809, -1.9809],\n",
       "           ...,\n",
       "           [-1.8782, -1.8953, -1.9124,  ..., -2.0665, -2.0665, -2.0494],\n",
       "           [-1.9467, -1.9467, -1.9638,  ..., -2.0665, -2.0665, -2.0494],\n",
       "           [-1.9809, -1.9638, -1.9809,  ..., -2.0665, -2.0665, -2.0665]],\n",
       " \n",
       "          [[-1.8606, -1.8782, -1.8957,  ..., -1.9307, -1.9307, -1.9307],\n",
       "           [-1.8782, -1.8606, -1.8606,  ..., -1.9307, -1.9132, -1.8957],\n",
       "           [-1.8606, -1.8431, -1.8431,  ..., -1.9132, -1.8957, -1.8957],\n",
       "           ...,\n",
       "           [-1.7906, -1.8081, -1.8256,  ..., -1.9832, -1.9832, -1.9657],\n",
       "           [-1.8606, -1.8606, -1.8782,  ..., -1.9832, -1.9832, -1.9657],\n",
       "           [-1.8957, -1.8782, -1.8957,  ..., -1.9832, -1.9832, -1.9832]],\n",
       " \n",
       "          [[-1.6302, -1.6476, -1.6650,  ..., -1.6999, -1.6999, -1.6999],\n",
       "           [-1.6476, -1.6302, -1.6302,  ..., -1.6999, -1.6824, -1.6650],\n",
       "           [-1.6302, -1.6127, -1.6127,  ..., -1.6824, -1.6650, -1.6650],\n",
       "           ...,\n",
       "           [-1.5604, -1.5779, -1.5953,  ..., -1.7522, -1.7522, -1.7347],\n",
       "           [-1.6302, -1.6302, -1.6476,  ..., -1.7522, -1.7522, -1.7347],\n",
       "           [-1.6650, -1.6476, -1.6650,  ..., -1.7522, -1.7522, -1.7522]]]]),\n",
       " 'caption': 'author : a life in photography -- in pictures',\n",
       " 'input_ids': tensor([[50265, 50265,     2, 11515,  4832,    10,   301,    11, 11075,   480,\n",
       "             11,  3493]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'image_token_mask': tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'item_id': 0,\n",
       " 'image_id': -1,\n",
       " 'caption_id': -1,\n",
       " 'raw_image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=736x736>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.image_token_id\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08569c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(50118))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8273efb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image> <image>Image of life in photography with cinematic tone<image> <image>What is this image?\n",
      "[(tensor(2), '</s>'), (tensor(50265), '<image>'), (tensor(50265), '<image>'), (tensor(8532), 'Image'), (tensor(9), ' of'), (tensor(301), ' life'), (tensor(11), ' in'), (tensor(11075), ' photography'), (tensor(19), ' with'), (tensor(25306), ' cinematic'), (tensor(6328), ' tone'), (tensor(50265), '<image>'), (tensor(50265), '<image>'), (tensor(2264), 'What'), (tensor(16), ' is'), (tensor(42), ' this'), (tensor(2274), ' image'), (tensor(116), '?')]\n",
      "50265\n",
      "tensor([[False,  True,  True, False, False, False, False, False, False, False,\n",
      "         False,  True,  True, False, False, False, False, False]])\n",
      "torch.Size([2, 3, 224, 224])\n",
      "tensor([[-0.0971, -0.0236,  0.0179,  ...,  0.1426, -0.0822,  0.0392],\n",
      "        [-0.0383,  0.0132,  0.0381,  ...,  0.0201,  0.0230,  0.0150]],\n",
      "       device='cuda:0', dtype=torch.float16) torch.Size([2, 1024])\n",
      "BeamSearchDecoderOnlyOutput(sequences=tensor([[    2, 50265, 50265,  8532,     9,   301,    11, 11075,    19, 25306,\n",
      "          6328, 50265, 50265,  2264,    16,    42,  2274,   116, 50118, 50118]],\n",
      "       device='cuda:0'), sequences_scores=None, scores=None, beam_indices=None, attentions=None, hidden_states=None)\n",
      "Image of life in photography with cinematic toneWhat is this image?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Few shot\n",
    "item0 = data[0]\n",
    "prompt = ' '.join([IMAGE_TOKEN for i in range(num_image_tokens)]) + 'Image of life in photography with cinematic tone'\n",
    "item = data[1] # image\n",
    "prompt += ' '.join([IMAGE_TOKEN for i in range(num_image_tokens)]) + 'What is this image?'\n",
    "print(prompt)\n",
    "tokens = data.tokenizer(prompt)\n",
    "input_ids = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
    "decoded_pairs = [(token_id, data.tokenizer.decode([token_id])) for token_id in input_ids[0]]\n",
    "print(decoded_pairs)\n",
    "attention_mask = torch.tensor(tokens['attention_mask']).unsqueeze(0)\n",
    "# attention_mask = torch.tensor([1, 1, 1]).unsqueeze(0)\n",
    "# inputs = data.tokenizer(prompt, return_tensors='pt')\n",
    "image_token_id = data.tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n",
    "print(image_token_id)\n",
    "image_token_mask = input_ids == image_token_id\n",
    "print(image_token_mask)\n",
    "\n",
    "kwargs = {\n",
    "    'pixel_values': torch.concat([item0['pixel_values'].half().to(device), item['pixel_values'].half().to(device)], axis=0),\n",
    "    'input_ids': input_ids.to(device),\n",
    "    'attention_mask': attention_mask.to(device),\n",
    "    'image_token_mask': image_token_mask.long().to(device),\n",
    "    'num_beams': 5,\n",
    "}\n",
    "with torch.no_grad():\n",
    "    experiment.model.eval()\n",
    "    output = experiment.model.generate(**kwargs)\n",
    "\n",
    "print(output)\n",
    "\n",
    "decoded = tokenizer.batch_decode(\n",
    "    output.sequences,  # filter out None values\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")\n",
    "\n",
    "# display(item['raw_image'])\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "860ff0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ae626b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([item['pixel_values'].half().to(device), item['pixel_values'].half().to(device)], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8e947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python (frozen)",
   "language": "python",
   "name": "frozen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
